---
title: "Project_report"
author: "Baldeep Dhada,Han Chen,Somya Nagar"
date: "2024-03-21"
output: pdf_document
---

## Introduction and Motivation

Cardiovascular diseases (CVDs) are a group of disorders of the heart and blood vessels. Heart attacks and strokes are usually acute events and are mainly caused by a blockage that prevents blood from flowing to the heart or brain. The most common reason for this is a build-up of fatty deposits on the inner walls of the blood vessels that supply the heart or brain. Strokes can be caused by bleeding from a blood vessel in the brain or from blood clots.

We analysed a dataset collected on patients with left ventricular systolic dysfunction in April-December 2015 at the Faisalabad Institute of Cardiology and at the Allied Hospital in Faisalabad (Punjab, Pakistan). The data is from patients that are over 40 years old. The data was collected by cardiac echo report or notes written by the physician. The data set providers changed the names of some of the features of the original data set which can introduce a bias in the report. An underlying scientific process that may affect the data is that high blood pressure was not reported and the type of the follow up is not reported as well. This can affect the data analysis because those two variables are crucial to understanding heart disease.

With the analysis conducted on this data, we hope to answer some important scientific questions that will help in a better understanding, and the treatment of cardiovascular diseases.

## Data description

### 1. Raw data

This dataset includes the medical histories of 299 patients diagnosed with heart failure, with the target variable being 'death_event'. As the *table 1* shows each patient's profile consists of 13 clinical attributes, 5 categorical variables and 7 numerical variables.

| variables                | Type       | description                                                   | values         |
|------------------|------------------|--------------------|------------------|
| age                      | Integer    | age of the patient (years)                                    | [40,95]        |
| anaemia                  | Binary     | decrease of red blood cells or hemoglobin                     | 0,1            |
| creatinine_phosphokinase | Integer    | level of the CPK enzyme in the blood(mcg/L)                   | [23,7861]      |
| diabetes                 | Binary     | if the patient has diabetes                                   | 0,1            |
| ejection_fraction        | Integer    | percentage of blood leaving the heart at each contraction (%) | [14,80]        |
| high_blood_pressure      | Binary     | if the patient has hypertension                               | 0,1            |
| platelets                | Continuous | platelets in the blood(kiloplatelets/mL)                      | [25100,850000] |
| serum_creatinine         | Continuous | level of serum creatinine in the blood (mg/dL)                | [0.5,9.4]      |
| serum_sodium             | Integer    | level of serum sodium in the blood (mEq/L)                    | [113,148]      |
| sex                      | Binary     | woman or man                                                  | 0,1            |
| smoking                  | Binary     | if the patient smokes or not                                  | 0,1            |
| time                     | Integer    | follow-up period (days)                                       | [4,285]        |
| death_event              | Binary     | if the patient died during the follow-up period               | 0,1            |

[Table 1. A table provides a description of the variables included in the dataset. The variables are categorized by type, including Integer, Binary, and Continuous, along with a brief description of each variable's meaning. The 'values' column indicates the range or possible values for each variable.]{width="321"}

1.1 Target variable

As Figure 1 shows below, our targeted variables `death_event` is unbalanced with 203 survival observations vs 96 dead observations.

![A pie chart depicting the distribution of survival and mortality rates, crucial target variables in our analysis. There are 203 survival observations and only 96 dead observations signifying imbalance in the data.](image/pie.png){width="321"}

1.2 Numerical variables

As the Figure 2 shows, these variables exhibit different scales, indicating the need for standardization. Based on descriptive statistics, the variables do not appear to have significant differences between the deceased and surviving groups, except for **`time`**.

![Statistical quantitative description of the numeric features that shows the mean, median and standard deviation for each of the quantitative variables. The mean, median and standard deviation values are further grouped into 'Full sample', 'Dead patients' and 'Survived Patients' to give a description based on the target variable.](image/table.png)

1.3 Categorical variables

We visualize 5 categorical variables against the target variable "death_event." As Figure 3 shows below, the number of males is larger than females, and individuals with a history of 'anaemia,' 'high blood pressure,' 'diabetes,' and 'smoking' are more prevalent than those without such histories. These variables demonstrate bias. Therefore, we need to perform oversampling to preprocess our data.

![Heatmap of Categorical Variables (high_blood_pressure, smoking, anemia, diabetes, sex) that shows the trend of counts of records with the target variable death_event. Count of records can be seen for 'Death' and 'Survival' for each of the variables.](image/discrete.png){width="609" height="149"}

### 2. Data Preprocessing

[**2.1 Standardization**]{.underline}

Based on Table 1 and Figure 2, the numerical variables have vastly different scales, so we need to scale our data to prevent some features with larger scales from dominating the learning process. Also, we can observe the presence of outliers in the data. Therefore, choosing Z-score Normalization would be a suitable method for standardization. Additionally, from the distribution of continuous variables, it is evident that the distribution of data varies with different values of the target variable for some continuous variables. Z-score Normalization can effectively preserve the information about the original variable distributions.

Mathematically, for a feature $X$, the Z-score normalization formula is:

$$ Z = \frac{(X - \mu)}{\sigma} $$

Where:

-   $Z$ is the standardized value (Z-score).

-   $X$ is the original data point.

-   $\mu$ is the mean of the feature.

-   $\sigma$ is the standard deviation of the feature.

The resulting Z-scores have a mean of 0 and a standard deviation of 1, making it easier to compare and interpret different features with varying scales.

[**2.2 Oversampling**]{.underline}

As Figure 1 shows, there is a huge imbalance in the response variable `death_event` with 203 survival observations vs 96 dead observations. Dealing with imbalanced datasets poses a challenge because the majority of machine learning methods tend to overlook the minority class, resulting in poor prediction. Hence, we will attempt to balance using the SMOTE (Synthetic Minority Oversampling Technique) algorithm.

SMOTE works by synthesizing new samples of the minority class to balance the disparate sample sizes between different classes. The core idea of the SMOTE algorithm is to generate synthetic samples by interpolating between existing minority class samples, thus increasing the quantity of minority class samples and creating a more balanced distribution between the minority and majority classes.

As shown in Figure 4, even after oversampling, each category of response remains equal (203 occurrences each), yet the distribution of categorical variables remains uneven, except for the variable 'anaemia'

![Oversampling Heatmap of Categorical Variables (high_blood_pressure, smoking, anemia, diabetes, sex) depicting the counts of records with the target variable death_event. Counts of records can be observed for 'Death' and 'Survival' for each of the variables.](image/oversampling.png){width="609" height="149"}

## Scientific Hypotheses Discussion

The medical dataset at hand contains valuable information and can help in answering some crucial questions, and understanding trends that provide useful insights about patient's health. It has great applications in a predictive landscape, and we will try to use the predictors and create models to make predictions.

Using this dataset, we will start our analysis by understanding the different factors and how they influence our understanding of cardiovascular diseases. We will primarily focus on finding out what predictors are the most relevant in predicting patients' death due to a heart attack. Then we will also attempt to predict whether the patient dies in the follow up period by leveraging predictive models and machine learning algorithms.

This should hold numerous applications in the medical landscape. The scientific questions that we will attempt to answer using our analysis are discussed below:

**1. Out of all the predictors available, which of them are significant in determining whether the patient dies within the follow up period?**

-   Identifying the key clinical variables associated with the risk of death is crucial for understanding disease progression and tailoring treatment plans. It can guide clinicians in prioritizing interventions and monitoring strategies for at-risk patients.

-   Heart failure is characterized by complex mechanisms and is affected differently by different clinical factors. Hence, having a better understanding of what predictors are significant would prove to be vital information to any physician.

-   Having the answer to this question would have a great significance in the problem that we are trying to solve with this project.

**2. Can we predict the likelihood of death during the follow-up period based on the patient's clinical attributes?**

-   Heart failure is a condition that presents with various complications and clinical presentations. Predicting the likelihood of death due to heart failure during the follow-up period can prove to be crucial.

-   This question is of great importance in clinical practice as it helps healthcare providers identify patients at higher risk of mortality, allowing for timely interventions and personalized treatment strategies.

-   By identifying patients at higher risk of mortality, healthcare providers can implement targeted interventions and monitor the disease progression more closely. It will also have applications in adjusting the follow-up period of the patients accordingly.

# Feature selection

As shown in Figure 5, there exists some correlation between variables. We will conduct feature selection to identify the most important variables for model prediction.

![A correlation Matrix that shows the collinearity between the predictors and the reponse variable. A value close to -1 or 1 is indicative of high collinearity between predictors. A value close to 0 is indicative of low collinearity between predictors. The more saturated a box is relative to its color the higher the collinearity. The less saturated a box is relative to its color the lower the collinearity.](image/correlative%20matrix.png)

### 1. Z-test

A Z-test is a statistical method used to assess whether there's a significant difference between the means of the populations under consideration. By formulating the null hypothesis (`H0`) suggests that the population means are equal. Next, the test statistic (`Z`) is calculated using the formula:

$$ Z = \frac{{\bar{X}_1 - \bar{X}_2}}{{\sqrt{\frac{{\sigma_1^2}}{{n_1}} + \frac{{\sigma_2^2}}{{n_2}}}}} $$

Here, $\bar{X}_1$ and $\bar{X}_2$ represent the sample means of the two populations, while $\sigma_1$ and $\sigma_2$ denote the known population standard deviations. $n_1$ and $n_2$ are the sample sizes for the respective populations.

As we can see on the table 3, variable `time` has significant high z-score and `ejection_fraction`, `age` and `serum_creatinine` are the following important variables

### 2. LASSO

LASSO (Least Absolute Shrinkage and Selection Operator) is a statistical formula whose main purpose is the feature selection and regularization of data models. In statistical learning space, a linear model is built by adding coefficients to each predictor. The LASSO method shrinks those regression coefficients, reducing some of them to zero. The feature selection phase occurs after the shrinkage, where every non-zero value is selected to be used in the model.

The features selection phase helps in the selection of explanatory variables, which are the input variables in the model. The input variables are important because they are elements that determine the model's output.

To answer this scientific question, we need to find out what predictors, or input variables from our data are important in determining the output variable. The LASSO technique can help in answering this question. The results of the LASSO model are showed in the figure below:

![](image/LASSO-1.png)

The LASSO model suggests these features to be important : `age`, `ejection_fraction`, `serum_sodium` and `time`.

### 3. Decision Trees

Decision trees is another method that can be used for classification problems. For this process, we create a model that predicts the value of the target variable by simple decision rules from the input data / features. At each step, there is a decision involved and then you follow consecutive decisions until you reach a value for the target variable.

Decision trees are fairly easy to interpret. The process of classification using decision trees feels very intuitive and is very easy to understand by simply looking at the tree structure. The deeper the tree, the more complex the decision rules and the fitter the model.

We can use this process to answer the first scientific question about which features are important as well. The input variables / features that are used the most in making these decisions in the trees are considered as most significant in determining the target variable. In this way, decision trees can help us in feature selection. The results for feature importance from the decision tree can be seen from the plot below:

![](image/DT-1.png)

# Model

Extreme Gradient Boosting (XGBoost) is an ensemble learning method that employs gradient boosting to train decision trees sequentially. The optimal parameters obtained through grid search are ('colsample_bytree': 0.6, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150, 'subsample': 0.8)

![A confusion matrix displaying the accuracy of the boosting model. The true label refers to the actual class of the data point being evaluated. The predicted label refers to the class that the model assigns to a particular data point based on its features. 0 refers to patient survival and 1 refers to patient death. The model correctly identified 46 true negatives (survival) and 33 true positives (deaths). The model has 2 false positives (misidentified 2 deaths as survival) and 1 false negative (misidentified 1 survival as a death). The color of the squares represent the rate of the classification in each category. A higher rate will be closer to a yellow color and a lower rate will be closer to dark purple color.](image/GB%20CM.png)

Generalized Additive Models (GAMs) are a class of generalized linear models that allow for flexible modeling of non-linear relationships. The model utilized B-spline basis functions with the number of degrees of freedom (df) and degrees of the spline set to [3, 3, 4, 4, 4, 3, 4] and [2, 2, 3, 3, 3, 2, 3], respectively, allowing for the capture of non-linear relationships between the predictors ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time'] and the response variable.

A Random Forest is a common strategy to improve CART predictions and stability. Classification Trees have high variability, in this random forest model we are fitting 100 trees then get the average prediction. The issue with this model is that roughly 1/3 of the observations will not appear while others are duplicated. These observations are called out of bag. We can report error using the observed data only through models fit with the observation was OOB.

![A confusion matrix displaying the accuracy of the random forest model. The true label refers to the actual class of the data point being evaluated. The predicted label refers to the class that the model assigns to a particular data point based on its features. 0 refers to patient survival and 1 refers to patient death. The model correctly identified 46 true negatives (survival) and 33 true positives (deaths). The model has 2 false positives (misidentified 2 deaths as survival) and 1 false negative (misidentified 1 survival as a death). The color of the squares represent the rate of the classification in each category. A higher rate will be closer to a yellow color and a lower rate will be closer to dark purple color.](image/GB%20CM.png){alt="A confusion matrix displaying the accuracy of the boosting model. The model correctly identified 46 true negatives (survival) and 33 true positives (deaths). The model has 2 false positives (misidentified 2 deaths as survival) and 1 false negative (misidentified 1 survival as a death). The color of the squares represent the rate of the classification in each category. A higher rate will be closer to a yellow color and a lower rate will be closer to dark purple color."}

# Result

| Model               | Accuracy | F1 Score | Precision | log loss |
|---------------------|----------|----------|-----------|----------|
| Random Forest       | 96.3%    | 0.97     | 0.96      | 0.192    |
| Boosting            | 96.3%    | 0.97     | 0.96      | 0.209    |
| Logistic Regression | 82.9%    | 0.84     | 0.88      | 0.427    |

: This table represents the performance of the models. It showcases the accuracy of the model which is the rate of accurate predictions. Precision measures the accuracy of positive predictions. It is the ratio of true positive predictions to the total number of positive predictions made by the model. Recall, also known as sensitivity or true positive rate, measures the ability of the model to correctly identify positive instances. It is the ratio of true positive predictions to the total number of actual positive instances in the data. F1 score is the harmonic mean of precision and recall and provides a balance between these two metrics. The F1 score and precision reach their best value at 1 (perfect precision and recall) and worst at 0. Log Loss is a measure used to evaluate the performance of a classification model that outputs probabilities. It measures the performance of a classification model where the prediction output is a probability value between 0 and 1. A lower log loss indicates better performance, with log loss approaching 0 for perfect predictions.
